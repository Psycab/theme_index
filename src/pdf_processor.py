"""
PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ëª¨ë“ˆ (GPU ê°€ì† ìµœì í™” ë²„ì „)
PDF ë¬¸ì„œì—ì„œ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
"""

import logging
import requests
import PyPDF2
import pdfplumber
import pytesseract
from PIL import Image
import io
import re
from typing import List, Optional, Dict
from dataclasses import dataclass
import time
from urllib.parse import urlparse
import hashlib
import pickle
import os
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import lru_cache
import threading
from queue import Queue
import json

# GPU OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    import easyocr
    import torch
    GPU_OCR_AVAILABLE = torch.cuda.is_available()
    logger = logging.getLogger(__name__)
    if GPU_OCR_AVAILABLE:
        logger.info("GPU OCR ì§€ì› ê°€ëŠ¥ (CUDA ì‚¬ìš© ê°€ëŠ¥)")
    else:
        logger.info("GPU OCR ì§€ì› ë¶ˆê°€ (CUDA ì‚¬ìš© ë¶ˆê°€)")
except ImportError as e:
    easyocr = None
    torch = None
    GPU_OCR_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"EasyOCR ë˜ëŠ” PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: {str(e)}. GPU OCR ë¹„í™œì„±í™”")

logger = logging.getLogger(__name__)

@dataclass
class ExtractedText:
    """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""
    document_id: str
    title: str
    raw_text: str
    processed_text: str
    extraction_method: str
    confidence_score: float
    page_count: int
    error_message: Optional[str] = None

class PDFProcessor:
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í”„ë¡œì„¸ì„œ (GPU ê°€ì† ìµœì í™” ë²„ì „)"""
    
    def __init__(self, timeout: int = 30, max_retries: int = 3, cache_dir: str = "cache/pdf_cache", 
                 use_gpu: bool = True, gpu_ocr_method: str = 'easyocr'):
        self.timeout = timeout
        self.max_retries = max_retries
        self.cache_dir = cache_dir
        self.use_gpu = use_gpu and GPU_OCR_AVAILABLE
        self.gpu_ocr_method = gpu_ocr_method
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        # ì„¸ì…˜ ì¬ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # GPU OCR ì´ˆê¸°í™”
        self.gpu_ocr_reader = None
        if self.use_gpu:
            self._init_gpu_ocr()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_processed': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'extraction_failures': 0,
            'gpu_ocr_used': 0,
            'cpu_ocr_used': 0,
            'total_time': 0.0
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
        self.stats_lock = threading.Lock()
        
        logger.info(f"GPU ê°€ì† PDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ (ìºì‹œ: {cache_dir}, GPU: {self.use_gpu})")
    
    def _init_gpu_ocr(self):
        """GPU OCR ì´ˆê¸°í™”"""
        try:
            if self.gpu_ocr_method == 'easyocr' and easyocr:
                logger.info("EasyOCR GPU ì´ˆê¸°í™” ì¤‘...")
                self.gpu_ocr_reader = easyocr.Reader(['ko', 'en'], gpu=True)
                logger.info("EasyOCR GPU ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” GPU OCR ë°©ë²•: {self.gpu_ocr_method}")
                self.use_gpu = False
        except Exception as e:
            logger.error(f"GPU OCR ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.use_gpu = False
            self.gpu_ocr_reader = None
    
    def _get_cache_key(self, url: str) -> str:
        """URLì„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _get_cache_path(self, url: str) -> str:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        cache_key = self._get_cache_key(url)
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def _load_from_cache(self, url: str) -> Optional[ExtractedText]:
        """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            cache_path = self._get_cache_path(url)
            if os.path.exists(cache_path):
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                
                # ìºì‹œ ë§Œë£Œ í™•ì¸ (24ì‹œê°„)
                if time.time() - cached_data.get('timestamp', 0) < 86400:
                    with self.stats_lock:
                        self.stats['cache_hits'] += 1
                    logger.debug(f"ìºì‹œ íˆíŠ¸: {url}")
                    return cached_data['extracted_text']
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                    os.remove(cache_path)
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {str(e)}")
        
        with self.stats_lock:
            self.stats['cache_misses'] += 1
        return None
    
    def _save_to_cache(self, url: str, extracted_text: ExtractedText):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        try:
            cache_path = self._get_cache_path(url)
            cache_data = {
                'extracted_text': extracted_text,
                'timestamp': time.time(),
                'url': url
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            logger.debug(f"ìºì‹œ ì €ì¥: {url}")
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {str(e)}")
    
    def download_pdf(self, url: str) -> Optional[bytes]:
        """PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type:
                logger.warning(f"PDFê°€ ì•„ë‹Œ íŒŒì¼: {url}, Content-Type: {content_type}")
                return None
                
            return response.content
            
        except requests.exceptions.RequestException as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def extract_text_pypdf2(self, pdf_content: bytes) -> tuple[str, int]:
        """PyPDF2ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            page_count = len(pdf_reader.pages)
            
            for page_num in range(page_count):
                try:
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page_num} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            return text.strip(), page_count
            
        except Exception as e:
            logger.error(f"PyPDF2 í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return "", 0
    
    def extract_text_pdfplumber(self, pdf_content: bytes) -> tuple[str, int]:
        """pdfplumberë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = io.BytesIO(pdf_content)
            
            with pdfplumber.open(pdf_file) as pdf:
                text = ""
                page_count = len(pdf.pages)
                
                for page_num, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except Exception as e:
                        logger.warning(f"í˜ì´ì§€ {page_num} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                return text.strip(), page_count
                
        except Exception as e:
            logger.error(f"pdfplumber í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return "", 0
    
    def extract_text_ocr(self, pdf_content: bytes) -> tuple[str, int]:
        """OCRì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)"""
        try:
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            page_count = len(pdf_reader.pages)
            
            for page_num in range(page_count):
                try:
                    page = pdf_reader.pages[page_num]
                    
                    # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                    page_obj = page
                    if hasattr(page_obj, 'get_images'):
                        # ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
                        images = page_obj.get_images()
                        if images:
                            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì²˜ë¦¬
                            img_obj = images[0]
                            img_data = pdf_reader.get_object(img_obj[0])
                            img_bytes = img_data.get_data()
                            
                            # PIL Imageë¡œ ë³€í™˜
                            img = Image.open(io.BytesIO(img_bytes))
                            
                            # OCR ìˆ˜í–‰
                            page_text = pytesseract.image_to_string(img, lang='kor+eng')
                            if page_text:
                                text += page_text + "\n"
                    
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page_num} OCR ì‹¤íŒ¨: {str(e)}")
                    continue
            
            return text.strip(), page_count
            
        except Exception as e:
            logger.error(f"OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return "", 0
    
    def extract_text_easyocr_gpu(self, pdf_content: bytes) -> tuple[str, int]:
        """EasyOCR GPUë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            if not self.gpu_ocr_reader:
                logger.warning("GPU OCR ë¦¬ë”ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return "", 0
            
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            page_count = len(pdf_reader.pages)
            
            for page_num in range(page_count):
                try:
                    page = pdf_reader.pages[page_num]
                    
                    # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                    page_obj = page
                    if hasattr(page_obj, 'get_images'):
                        images = page_obj.get_images()
                        if images:
                            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì²˜ë¦¬
                            img_obj = images[0]
                            img_data = pdf_reader.get_object(img_obj[0])
                            img_bytes = img_data.get_data()
                            
                            # PIL Imageë¡œ ë³€í™˜
                            img = Image.open(io.BytesIO(img_bytes))
                            
                            # EasyOCR GPUë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            results = self.gpu_ocr_reader.readtext(img)
                            page_text = ' '.join([result[1] for result in results])
                            if page_text:
                                text += page_text + "\n"
                    
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page_num} GPU OCR ì‹¤íŒ¨: {str(e)}")
                    continue
            
            return text.strip(), page_count
            
        except Exception as e:
            logger.error(f"EasyOCR GPU í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return "", 0
    
    def extract_text_from_pdf(self, url: str) -> ExtractedText:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìºì‹± + ë‹¤ì¤‘ ë°©ë²• ì‹œë„)"""
        start_time = time.time()
        
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached_result = self._load_from_cache(url)
        if cached_result:
            logger.debug(f"ìºì‹œì—ì„œ ë°˜í™˜: {url}")
            return cached_result
        
        logger.info(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {url}")
        
        # PDF ë‹¤ìš´ë¡œë“œ
        pdf_content = self.download_pdf(url)
        if not pdf_content:
            result = ExtractedText(
                document_id="",
                title="",
                raw_text="",
                processed_text="",
                extraction_method="download_failed",
                confidence_score=0.0,
                page_count=0,
                error_message="PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
            )
            with self.stats_lock:
                self.stats['extraction_failures'] += 1
            return result
        
        # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•)
        extraction_methods = [
            ("pdfplumber", self.extract_text_pdfplumber),
            ("pypdf2", self.extract_text_pypdf2),
            ("ocr", self.extract_text_ocr)
        ]
        
        # GPU OCRì„ ì¶”ì¶œ ë°©ë²•ì— ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.use_gpu and self.gpu_ocr_reader:
            extraction_methods.append(("easyocr_gpu", self.extract_text_easyocr_gpu))
        
        best_text = ""
        best_method = ""
        best_page_count = 0
        best_confidence = 0.0
        
        for method_name, extract_func in extraction_methods:
            try:
                text, page_count = extract_func(pdf_content)
                
                if text and len(text.strip()) > 50:  # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
                    # í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                    confidence = self._calculate_text_confidence(text)
                    
                    if confidence > best_confidence:
                        best_text = text
                        best_method = method_name
                        best_page_count = page_count
                        best_confidence = confidence
                        
                    logger.info(f"{method_name} ì„±ê³µ: {len(text)}ì, ì‹ ë¢°ë„: {confidence:.2f}")
                    
                    # GPU OCR ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
                    if method_name == "easyocr_gpu":
                        with self.stats_lock:
                            self.stats['gpu_ocr_used'] += 1
                    elif method_name == "ocr":
                        with self.stats_lock:
                            self.stats['cpu_ocr_used'] += 1
                else:
                    logger.warning(f"{method_name} ê²°ê³¼ ë¶€ì¡±: {len(text)}ì")
                    
            except Exception as e:
                logger.error(f"{method_name} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                continue
        
        if not best_text:
            return ExtractedText(
                document_id="",
                title="",
                raw_text="",
                processed_text="",
                extraction_method="all_failed",
                confidence_score=0.0,
                page_count=0,
                error_message="ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨"
            )
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = self._preprocess_text(best_text)
        
        result = ExtractedText(
            document_id="",
            title="",
            raw_text=best_text,
            processed_text=processed_text,
            extraction_method=best_method,
            confidence_score=best_confidence,
            page_count=best_page_count
        )
        
        # ìºì‹œì— ì €ì¥
        self._save_to_cache(url, result)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        with self.stats_lock:
            self.stats['total_processed'] += 1
            self.stats['total_time'] += processing_time
        
        logger.info(f"PDF ì¶”ì¶œ ì™„ë£Œ: {url} ({best_method}, {processing_time:.2f}ì´ˆ)")
        return result
    
    def _calculate_text_confidence(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not text:
            return 0.0
        
        confidence = 0.0
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì ìˆ˜ (0-0.3)
        text_length = len(text.strip())
        if text_length > 1000:
            confidence += 0.3
        elif text_length > 500:
            confidence += 0.2
        elif text_length > 100:
            confidence += 0.1
        
        # í•œê¸€ ë¹„ìœ¨ ì ìˆ˜ (0-0.3)
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.findall(r'[ê°€-í£a-zA-Z0-9]', text))
        if total_chars > 0:
            korean_ratio = korean_chars / total_chars
            confidence += korean_ratio * 0.3
        
        # ë¬¸ì¥ êµ¬ì¡° ì ìˆ˜ (0-0.2)
        sentences = re.split(r'[.!?]', text)
        avg_sentence_length = sum(len(s.strip()) for s in sentences) / len(sentences) if sentences else 0
        if 10 <= avg_sentence_length <= 100:
            confidence += 0.2
        elif 5 <= avg_sentence_length <= 150:
            confidence += 0.1
        
        # íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨ ì ìˆ˜ (0-0.2)
        special_chars = len(re.findall(r'[^\w\sê°€-í£]', text))
        if total_chars > 0:
            special_ratio = special_chars / total_chars
            if special_ratio < 0.1:  # íŠ¹ìˆ˜ë¬¸ìê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
                confidence += 0.2
            elif special_ratio < 0.2:
                confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ê¸°ë³¸ ì •ë¦¬
        text = text.strip()
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n+', '\n', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (ì¼ë¶€ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£.,!?;:()\[\]{}""''-]', ' ', text)
        
        # ì—°ì†ëœ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        text = re.sub(r'[.,!?;:]{2,}', '.', text)
        
        return text.strip()
    
    def batch_extract_texts(self, documents: List[Dict], 
                           max_workers: int = 5) -> List[ExtractedText]:
        """ì—¬ëŸ¬ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì¶”ì¶œ"""
        logger.info(f"{len(documents)}ê°œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘")
        
        extracted_texts = []
        
        for i, doc in enumerate(documents):
            try:
                url = doc.get('url', '')
                if not url:
                    logger.warning(f"ë¬¸ì„œ {i} URL ì—†ìŒ")
                    continue
                
                extracted = self.extract_text_from_pdf(url)
                extracted.document_id = doc.get('document_id', '')
                extracted.title = doc.get('title', '')
                
                extracted_texts.append(extracted)
                
                logger.info(f"ë¬¸ì„œ {i+1}/{len(documents)} ì²˜ë¦¬ ì™„ë£Œ: {extracted.extraction_method}")
                
                # API ì œí•œ ë°©ì§€
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"ë¬¸ì„œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
        
        logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_texts)}ê°œ ë¬¸ì„œ")
        return extracted_texts
    
    def high_performance_batch_extract(self, documents: List[Dict], 
                                     max_workers: int = None, 
                                     batch_size: int = 50) -> List[ExtractedText]:
        """ê³ ì„±ëŠ¥ ë³‘ë ¬ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if max_workers is None:
            max_workers = min(32, (os.cpu_count() or 1) * 4)  # CPU ì½”ì–´ì˜ 4ë°°
        
        logger.info(f"ê³ ì„±ëŠ¥ ë³‘ë ¬ PDF ì¶”ì¶œ ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ, {max_workers}ê°œ ì›Œì»¤")
        start_time = time.time()
        
        extracted_texts = []
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ê° ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            future_to_batch = {
                executor.submit(self._process_batch, batch, batch_idx): batch_idx 
                for batch_idx, batch in enumerate(batches)
            }
            
            # ì§„í–‰ë¥  í‘œì‹œê¸°ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            from tqdm import tqdm
            with tqdm(total=len(batches), desc="PDF ì¶”ì¶œ", unit="ë°°ì¹˜", 
                      bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
                
                for future in as_completed(future_to_batch):
                    batch_idx = future_to_batch[future]
                    try:
                        batch_results = future.result()
                        extracted_texts.extend(batch_results)
                        pbar.set_postfix({
                            'ë¬¸ì„œìˆ˜': len(batch_results),
                            'ì´ë¬¸ì„œ': len(extracted_texts)
                        })
                        logger.info(f"ğŸ“„ ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ì™„ë£Œ: {len(batch_results)}ê°œ ë¬¸ì„œ")
                    except Exception as e:
                        logger.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    
                    pbar.update(1)
        
        total_time = time.time() - start_time
        logger.info(f"ê³ ì„±ëŠ¥ ë³‘ë ¬ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_texts)}ê°œ ë¬¸ì„œ, {total_time:.2f}ì´ˆ")
        
        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        self._log_performance_stats()
        
        return extracted_texts
    
    def _process_batch(self, batch: List[Dict], batch_idx: int) -> List[ExtractedText]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ PDF ì²˜ë¦¬"""
        batch_results = []
        
        for i, doc in enumerate(batch):
            try:
                url = doc.get('url', '')
                if not url:
                    logger.warning(f"ë°°ì¹˜ {batch_idx} ë¬¸ì„œ {i} URL ì—†ìŒ")
                    continue
                
                extracted = self.extract_text_from_pdf(url)
                extracted.document_id = doc.get('document_id', '')
                extracted.title = doc.get('title', '')
                
                batch_results.append(extracted)
                
                # ì ì‘ì  ì§€ì—° (API ì œí•œ ë°©ì§€)
                if i < len(batch) - 1:  # ë§ˆì§€ë§‰ ë¬¸ì„œê°€ ì•„ë‹Œ ê²½ìš°
                    time.sleep(0.05)  # ê¸°ì¡´ 0.1ì´ˆì—ì„œ 0.05ì´ˆë¡œ ë‹¨ì¶•
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_idx} ë¬¸ì„œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
        
        return batch_results
    
    def _log_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë¡œê¹…"""
        with self.stats_lock:
            stats = self.stats.copy()
        
        if stats['total_processed'] > 0:
            avg_time = stats['total_time'] / stats['total_processed']
            cache_hit_rate = stats['cache_hits'] / (stats['cache_hits'] + stats['cache_misses']) * 100
            
            logger.info("=== PDF ì²˜ë¦¬ ì„±ëŠ¥ í†µê³„ ===")
            logger.info(f"ì´ ì²˜ë¦¬ ë¬¸ì„œ: {stats['total_processed']}ê°œ")
            logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ/ë¬¸ì„œ")
            logger.info(f"ìºì‹œ íˆíŠ¸ìœ¨: {cache_hit_rate:.1f}% ({stats['cache_hits']}/{stats['cache_hits'] + stats['cache_misses']})")
            logger.info(f"ì¶”ì¶œ ì‹¤íŒ¨: {stats['extraction_failures']}ê°œ")
            logger.info(f"GPU OCR ì‚¬ìš©: {stats['gpu_ocr_used']}ê°œ")
            logger.info(f"CPU OCR ì‚¬ìš©: {stats['cpu_ocr_used']}ê°œ")
            if stats['gpu_ocr_used'] + stats['cpu_ocr_used'] > 0:
                gpu_ocr_ratio = stats['gpu_ocr_used'] / (stats['gpu_ocr_used'] + stats['cpu_ocr_used']) * 100
                logger.info(f"GPU OCR ë¹„ìœ¨: {gpu_ocr_ratio:.1f}%")
            logger.info("========================")
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        with self.stats_lock:
            stats = self.stats.copy()
        
        if stats['total_processed'] > 0:
            stats['average_time'] = stats['total_time'] / stats['total_processed']
            stats['cache_hit_rate'] = stats['cache_hits'] / (stats['cache_hits'] + stats['cache_misses']) * 100
        else:
            stats['average_time'] = 0.0
            stats['cache_hit_rate'] = 0.0
        
        return stats
    
    def clear_cache(self):
        """ìºì‹œ ì‚­ì œ"""
        try:
            import shutil
            if os.path.exists(self.cache_dir):
                shutil.rmtree(self.cache_dir)
                os.makedirs(self.cache_dir, exist_ok=True)
            logger.info("PDF ìºì‹œ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
